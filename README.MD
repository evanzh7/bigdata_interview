

##  数据仓库

1. 讲一下什么是维度表和事实表。用户资料表算是什么类型表。
2. 维度建模属于第几范式，让你对维度建模改进，有什么思路吗。
3. 了解数据血缘分析吗，让你实现的话有什么技术方案，感觉难点在哪。
4. 了解数据分层吗，讲一下分四层或者五层各有什么优劣。自己模拟一个场景，给出不同的方案。
5. 数据口径不一致的问题一般在什么情况下会发生，怎么避免。
6. 数据仓库模型设计

	- 星型模型：当所有维表都直接连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型.星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一定的冗余，如在地域维度表中，存在国家 A 省 B 的城市 C 以及国家 A 省 B 的城市 D 两条记录，那么国家 A 和省 B 的信息分别存储了两次，即存在冗余。
	- 雪花模型：当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “ 层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表
	- 优缺点比较：星型模型因为数据的冗余所以很多统计查询不需要做外部的连接，因此一般情况下效率比雪花型模型要高。星型结构不用考虑很多正规化的因素，设计与实现都比较简单。雪花型模型由于去除了冗余，有些统计就需要通过表的联接才能产生，所以效率不一定有星型模型高。正规化也是一种比较复杂的过程，相应的数据库结构设计、数据的 ETL、以及后期的维护都要复杂一些。因此在冗余可以接受的前提下，实际运用中星型模型使用更多，也更有效率。雪花模型加载数据集市，因此ETL操作在设计上更加复杂，而且由于附属模型的限制，不能并行化。星形模型加载维度表，不需要再维度之间添加附属模型，因此ETL就相对简单，而且可以实现高度的并行化。
7. 离线计算的时候，每天抽取增量，然后又与昨天的全量合并生成今天的全量，有哪些方法？（如果不是高版本的话，没有事务功能，无法开启的话处理） 如果开启事务会很慢，为什么会慢？有哪些更好的方法？ 说说事务的缺陷？ 其实主要考察的是对拉链表的理解
8. 数据仓库是如何搭建的
9. 数据仓库搭建模型
10. 应用层有多少维度？ 

	时间，事件，国家，语言，站点，渠道（一级渠道，二级渠道，三级渠道）等维度 如果将这些维度怎么放在同一个表中进行支持任意的组合？在秒级别计算出来提供业务以及前端使用。（参考麒麟的cube多维度预处理的思想）
11. 假如说想做一个全局排序的话，而且数据量特别大怎么办？排序肯定是放在一台机器上计算,但是这一台计算无法支持或者说计算超级慢，怎么办？
12. 多维分析，混合模型（星星模型和雪花模型混合，星型为主）
13. 三范式是？雪花模型和星型模型？
14. replication join和hash join介绍。还有打乱到多台reduce中的参数
15. kafka 的架构，包含了哪些角色？
16. kafka 的最小工作单元？
17. kafka 消息重复消费的问题？幂等怎么做的？
18. kafka ack 机制？集群中的ack 是怎么实现的？
16. zookeeper的架构
1. Hadoop集群可以运行的3个模式？

	>单机（本地）模式  
	伪分布式模式  
	全分布式模式  
2. 单机（本地）模式中的注意点？

	>在单机模式（standalone）中不会存在守护进程，所有东西都运行在一个JVM上。这里同样没有DFS，使用的是本地文件系统。单机模式适用于开发过程中运行MapReduce程序，这也是最少使用的一个模式。

3. 伪分布模式中的注意点？
	>伪分布式（Pseudo）适用于开发和测试环境，在这个模式中，所有守护进程都在同一台机器上运行。
4. VM是否可以称为Pseudo？
	>不是，两个事物，同时Pseudo只针对Hadoop。
5. 全分布模式又有什么注意点？
	>全分布模式通常被用于生产环境，这里我们使用N台主机组成一个Hadoop集群，Hadoop守护进程运行在每台主机之上。这里会存在Namenode运行的主机，Datanode运行的主机，以及task tracker运行的主机。在分布式环境下，主节点和从节点会分开。
6. Hadoop是否遵循UNIX模式？
	>是的，在UNIX用例下，Hadoop还拥有“conf”目录。
7. Hadoop安装在什么目录下？
	>Cloudera和Apache使用相同的目录结构，Hadoop被安装在cd/usr/lib/hadoop-0.20/。
8. Namenode、Job tracker和task tracker的端口号是？
	>Namenode，70；  
	Job tracker，30；  
	Task tracker，60。
9. Hadoop的核心配置是什么？
	>Hadoop的核心配置通过两个xml文件来完成：  
	1，hadoop-default.xml；  
	2，hadoop-site.xml。  
	这些文件都使用xml格式，因此每个xml中都有一些属性，包括名称和值，但是当下这些文件都已不复存在。
10. 那当下又该如何配置？
	>Hadoop现在拥有3个配置文件：  
	1，core-site.xml；  
	2，hdfs-site.xml；  
	3，mapred-site.xml。这些文件都保存在conf/子目录下。
11. RAM的溢出因子是？
溢出因子（Spill factor）是临时文件中储存文件的大小，也就是Hadoop-temp目录。
12. fs.mapr.working.dir只是单一的目录？
fs.mapr.working.dir只是一个目录。
13. hdfs-site.xml的3个主要属性？

	>dfs.name.dir决定的是元数据存储的路径以及DFS的存储方式（磁盘或是远端）  
	dfs.data.dir决定的是数据存储的路径  
	fs.checkpoint.dir用于第二Namenode  
14. 如何退出输入模式？
	>退出输入的方式有：1，按ESC；2，键入:q（如果你没有输入任何当下）或者键入:wq（如果你已经输入当下），并且按下Enter。
15. 当你输入hadoopfsck /造成“connection refused java exception’”时，系统究竟发生了什么？
这意味着Namenode没有运行在你的VM之上。
16. 我们使用Ubuntu及Cloudera，那么我们该去哪里下载Hadoop，或者是默认就与Ubuntu一起安装？
这个属于Hadoop的默认配置，你必须从Cloudera或者Edureka的dropbox下载，然后在你的系统上运行。当然，你也可以自己配置，但是你需要一个Linux box，Ubuntu或者是Red Hat。在Cloudera网站或者是Edureka的Dropbox中有安装步骤。
17. “jps”命令的用处？
这个命令可以检查Namenode、Datanode、Task Tracker、 Job Tracker是否正常工作。
18. 如何重启Namenode？
	>点击stop-all.sh，再点击start-all.sh。
键入sudo hdfs（Enter），su-hdfs （Enter），/etc/init.d/ha（Enter），及/etc/init.d/hadoop-0.20-namenode start（Enter）。
19. Fsck的全名？
	>全名是：File System Check。
20. 如何检查Namenode是否正常运行？
	>如果要检查Namenode是否正常工作，使用命令/etc/init.d/hadoop-0.20-namenode status或者就是简单的jps。
21. mapred.job.tracker命令的作用？
	>可以让你知道哪个节点是Job Tracker。
22. /etc /init.d命令的作用是？
	>/etc /init.d说明了守护进程（服务）的位置或状态，其实是LINUX特性，和Hadoop关系不大。
23. 如何在浏览器中查找Namenode？
	>如果你确实需要在浏览器中查找Namenode，你不再需要localhost:8021，Namenode的端口号是50070。
24. 如何从SU转到Cloudera？
	>从SU转到Cloudera只需要键入exit。
25. 启动和关闭命令会用到哪些文件？
	>Slaves及Masters。
26. Slaves由什么组成？
	>Slaves由主机的列表组成，每台1行，用于说明数据节点。
27. Masters由什么组成？
	>Masters同样是主机的列表组成，每台一行，用于说明第二Namenode服务器。
28. hadoop-env.sh是用于做什么的？
	>hadoop-env.sh提供了Hadoop中. JAVA_HOME的运行环境。
29. Master文件是否提供了多个入口？
	>是的你可以拥有多个Master文件接口。
30. Hadoop-env.sh文件当下的位置？
	>hadoop-env.sh现在位于conf。
31. 在Hadoop_PID_DIR中，PID代表了什么？
	>PID代表了“Process ID”。
32. /var/hadoop/pids用于做什么？
	>/var/hadoop/pids用来存储PID。
33. hadoop-metrics.properties文件的作用是？
	>hadoop-metrics.properties被用做“Reporting”，控制Hadoop报告，初始状态是“not to report”。
34. Hadoop需求什么样的网络？
	>Hadoop核心使用Shell（SSH）来驱动从节点上的服务器进程，并在主节点和从节点之间使用password-less SSH连接。
35. 全分布式环境下为什么需求password-less SSH？
	>这主要因为集群中通信过于频繁，Job Tracker需要尽可能快的给Task Tracker发布任务。
36. 这会导致安全问题吗？
	>完全不用担心。Hadoop集群是完全隔离的，通常情况下无法从互联网进行操作。与众不同的配置，因此我们完全不需要在意这种级别的安全漏洞，比如说通过互联网侵入等等。Hadoop为机器之间的连接提供了一个相对安全的方式。
37. SSH工作的端口号是？
	>SSH工作的端口号是NO.22，当然可以通过它来配置，22是默认的端口号。
38. SSH中的注意点还包括？
	>SSH只是个安全的shell通信，可以把它当做NO.22上的一种协议，只需要配置一个密码就可以安全的访问。
39. 为什么SSH本地主机需要密码？
	>在SSH中使用密码主要是增加安全性，在某些情况下也根本不会设置密码通信。
40. 如果在SSH中添加key，是否还需要设置密码？
	>是的，即使在SSH中添加了key，还是需要设置密码。
41. 假如Namenode中没有数据会怎么样？
	>没有数据的Namenode就不能称之为Namenode，通常情况下，Namenode肯定会有数据。
42. 当Job Tracker宕掉时，Namenode会发生什么？
	>当Job Tracker失败时，集群仍然可以正常工作，只要Namenode没问题。
43. 是客户端还是Namenode决定输入的分片？
	>这并不是客户端决定的，在配置文件中以及决定分片细则。
44. 是否可以自行搭建Hadoop集群？
	>是的，只要对Hadoop环境足够熟悉，你完全可以这么做。
45. 是否可以在Windows上运行Hadoop？
	>你最好不要这么做，Red Hat Linux或者是Ubuntu才是Hadoop的最佳操作系统。在Hadoop安装中，Windows通常不会被使用，因为会出现各种各样的问题。因此，Windows绝对不是Hadoop的推荐系统。

## 第一部分、十道海量数据处理面试题
1. 海量日志数据，提取出某日访问百度次数最多的那个IP。

	>首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法， 比如模1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map进行频率统计，然后再找出频率最大 的几个）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。
或者如下阐述（雪域之鹰）：
算法思想：分而治之+Hash
1.IP地址最多有2^32=4G种取值情况，所以不能完全加载到内存中处理；
2.可以考虑采用“分而治之”的思想，按照IP地址的Hash(IP)%1024值，把海量IP日志分别存储到1024个小文件中。这样，每个小文件最多包含4MB个IP地址；
3.对于每一个小文件，可以构建一个IP为key，出现次数为value的Hash map，同时记录当前出现次数最多的那个IP地址；
4.可以得到1024个小文件中的出现次数最多的IP，再依据常规的排序算法得到总体上出现次数最多的IP；

2. 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。
假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的10个查询串，要求使用的内存不能超过1G。
典型的Top K算法，还是在这篇文章里头有所阐述，详情请参见：十一、从头到尾彻底解析Hash表算法。
文中，给出的最终算法是：
第一步、先对这批海量数据预处理，在O（N）的时间内用Hash表完成统计（之前写成了排序，特此订正。July、2011.04.27）；
第二步、借助堆这个数据结构，找出Top K，时间复杂度为N‘logK。
即，借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别 和根元素进行对比所以，我们最终的时间复杂度是：O（N） + N’*O（logK），（N为1000万，N’为300万）。ok，更多，详情，请参考原文。
或者：采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。

3. 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。
方案：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。
如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结 点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。

4. 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。
还是典型的TOP K算法，解决方案如下：
方案1：
顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。
找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的 query_cout输出到文件中。这样得到了10个排好序的文件（记为）。
对这10个文件进行归并排序（内排序与外排序相结合）。
方案2：
一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。
方案3：
与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。

5.  给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？
方案1：可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。
遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,…,a999）中。这样每个小文件的大约为300M。
遍历文件b，采取和a相同的方式将url分别存储到1000小文件（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小 文件（a0vsb0,a1vsb1,…,a999vsb999）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的 url即可。
求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。
方案2：如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。
Bloom filter日后会在本BLOG内详细阐述。

6. 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。
方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看 bitmap，把对应位是01的整数输出即可。
方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。

7. 腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？
与上第6题类似，我的第一反应时快速排序+二分查找。以下是其它更好的方法：
方案1：oo，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。
dizengrong：
方案2：这个问题在《编程珠玑》里有很好的描述，大家可以参考下面的思路，探讨一下：
又因为2^32为40亿多，所以给定一个数可能在，也可能不在其中；
这里我们把40亿个数中的每一个用32位的二进制来表示
假设这40亿个数开始放在一个文件中。
然后将这40亿个数分成两类:
1.最高位为0
2.最高位为1
并将这两类分别写入到两个文件中，其中一个文件中数的个数<=20亿，而另一个>=20亿（这相当于折半了）；
与要查找的数的最高位比较并接着进入相应的文件再查找
再然后把这个文件为又分成两类:
1.次最高位为0
2.次最高位为1
并将这两类分别写入到两个文件中，其中一个文件中数的个数<=10亿，而另一个>=10亿（这相当于折半了）；
与要查找的数的次最高位比较并接着进入相应的文件再查找。
…….
以此类推，就可以找到了,而且时间复杂度为O(logn)，方案2完。
附：这里，再简单介绍下，位图方法：
使用位图法判断整形数组是否存在重复
判断集合中存在重复是常见编程任务之一，当集合中数据量比较大时我们通常希望少进行几次扫描，这时双重循环法就不可取了。
位图法比较适合于这种情况，它的做法是按照集合中最大元素max创建一个长度为max+1的新数组，然后再次扫描原数组，遇到几就给新数组的第几位置上 1，如遇到5就给新数组的第六个元素置1，这样下次再遇到5想置位时发现新数组的第六个元素已经是1了，这说明这次的数据肯定和以前的数据存在着重复。这 种给新数组初始化时置零其后置一的做法类似于位图的处理方法故称位图法。它的运算次数最坏的情况为2N。如果已知数组的最大值即能事先给新数组定长的话效 率还能提高一倍。
欢迎，有更好的思路，或方法，共同交流。

8. 怎么在海量数据中找出重复次数最多的一个？
方案1：先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。

9. 上千万或上亿数据（有重复），统计其中出现次数最多的钱N个数据。
方案1：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后就是取出前N个出现次数最多的数据了，可以用第2题提到的堆机制完成。

10. 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。
方案1：这题是考虑时间效率。用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度）。然后是找出出现最频繁的前10 个词，可以用堆来实现，前面的题中已经讲到了，时间复杂度是O(n*lg10)。所以总的时间复杂度，是O(n*le)与O(n*lg10)中较大的哪一 个。
附、100w个数中找出最大的100个数。
方案1：在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100w*lg100)。
方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100w*100)。
方案3：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的 要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。
致谢：http://www.cnblogs.com/youwang/。

## 第二部分、十个海量数据处理方法大总结
ok，看了上面这么多的面试题，是否有点头晕。是的，需要一个总结。接下来，本文将简单总结下一些处理海量数据问题的常见方法，而日后，本BLOG内会具体阐述这些方法。
下面的方法全部来自http://hi.baidu.com/yanxionglu/blog/博客，对海量数据的处理方法进行了一个一般性的总结，当然这些方法可能并不能完全覆盖所有的问题，但是这样的一些方法也基本可以处理绝大多数遇到的问题。下面的一些问题基本直接来源于公司的面试笔试题目，方法不一定最优，如果你有更好的处理方法，欢迎讨论。

1. Bloom filter
适用范围：可以用来实现数据字典，进行数据的判重，或者集合求交集
基本原理及要点：
对于原理来说很简单，位数组+k个独立hash函数。将 hash函数对应的值的位数组置1，查找时如果发现所有hash函数对应位都是1说明存在，很明显这个过程并不保证查找的结果是100%正确的。同时也不 支持删除一个已经插入的关键字，因为该关键字对应的位会牵动到其他的关键字。所以一个简单的改进就是 counting Bloom filter，用一个counter数组代替位数组，就可以支持删除了。
还有一个比较重要的问题，如何根据输入元素个数n，确定位数组m的大小及hash函数 个数。当hash函数个数k=(ln2)\*(m/n)时错误率最小。在错误率不大于E的情况下，m至少要等于n\*lg(1/E)才能表示任意n个元素的集 合。但m还应该更大些，因为还要保证bit数组里至少一半为0，则m应该>=nlg(1/E)\*lge 大概就是nlg(1/E)1.44倍(lg表示以2为底的对数)。
举个例子我们假设错误率为0.01，则此时m应大概是n的13倍。这样k大概是8个。
注意这里m与n的单位不同，m是bit为单位，而n则是以元素个数为单位(准确的说是不同元素的个数)。通常单个元素的长度都是有很多bit的。所以使用bloom filter内存上通常都是节省的。
扩展：
Bloom filter将集合中的元素映射到位数组中，用k（k为哈希函数个数）个映射位是否全1表示元素在不在这个集合中。Counting bloom filter（CBF）将位数组中的每一位扩展为一个counter，从而支持了元素的删除操作。Spectral Bloom Filter（SBF）将其与集合元素的出现次数关联。SBF采用counter中的最小值来近似表示元素的出现频率。
问题实例：给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？
根据这个问题我们来计算下内存的占用，4G=2^32大概是40亿*8大概是340 亿，n=50亿，如果按出错率0.01算需要的大概是650亿个bit。现在可用的是340亿，相差并不多，这样可能会使出错率上升些。另外如果这些 urlip是一一对应的，就可以转换成ip，则大大简单了。

2. Hashing
适用范围：快速查找，删除的基本数据结构，通常需要总数据量可以放入内存
基本原理及要点：
hash函数选择，针对字符串，整数，排列，具体相应的hash方法。
碰撞处理，一种是open hashing，也称为拉链法；另一种就是closed hashing，也称开地址法，opened addressing。
扩展：
d-left hashing中的d是多个的意思，我们先简化这个问题，看一看2-left hashing。2-left hashing指的是将一个哈希表分成长度相等的两半，分别叫做T1和T2，给T1和T2分别配备一个哈希函数，h1和h2。在存储一个新的key时，同 时用两个哈希函数进行计算，得出两个地址h1[key]和h2[key]。这时需要检查T1中的h1[key]位置和T2中的h2[key]位置，哪一个 位置已经存储的（有碰撞的）key比较多，然后将新key存储在负载少的位置。如果两边一样多，比如两个位置都为空或者都存储了一个key，就把新key 存储在左边的T1子表中，2-left也由此而来。在查找一个key时，必须进行两次hash，同时查找两个位置。
问题实例：
1).海量日志数据，提取出某日访问百度次数最多的那个IP。
IP的数目还是有限的，最多2^32个，所以可以考虑使用hash将ip直接存入内存，然后进行统计。
3. bit-map
适用范围：可进行数据的快速查找，判重，删除，一般来说数据范围是int的10倍以下
基本原理及要点：使用bit数组来表示某些元素是否存在，比如8位电话号码
扩展：bloom filter可以看做是对bit-map的扩展
问题实例：
1)已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。
8位最多99 999 999，大概需要99m个bit，大概10几m字节的内存即可。
2)2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。
将bit-map扩展一下，用2bit表示一个数即可，0表示未出现，1表示出现一次，2表示出现2次及以上。或者我们不用2bit来进行表示，我们用两个bit-map即可模拟实现这个2bit-map。
4. 堆
适用范围：海量数据前n大，并且n比较小，堆可以放入内存
基本原理及要点：最大堆求前n小，最小堆求前n大。方法，比如求前n小，我们比较当前 元素与最大堆里的最大元素，如果它小于最大元素，则应该替换那个最大元素。这样最后得到的n个元素就是最小的n个。适合大数据量，求前n小，n的大小比较 小的情况，这样可以扫描一遍即可得到所有的前n元素，效率很高。
扩展：双堆，一个最大堆与一个最小堆结合，可以用来维护中位数。
问题实例：
1)100w个数中找最大的前100个数。
用一个100个元素大小的最小堆即可。

5. 双层桶划分—-其实本质上就是【分而治之】的思想，重在“分”的技巧上！
适用范围：第k大，中位数，不重复或重复的数字
基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。可以通过多次缩小，双层只是一个例子。
扩展：
问题实例：
1).2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。
有点像鸽巢原理，整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。
2).5亿个int找它们的中位数。
这个例子比上面那个更明显。首先我们 将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第 几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。
实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受 的程度。即可以先将int64分成2^24个区域，然后确定区域的第几大数，在将该区域分成2^20个子区域，然后确定是子区域的第几大数，然后子区域里 的数的个数只有2^20，就可以直接利用direct addr table进行统计了。
6. 数据库索引
适用范围：大数据量的增删改查
基本原理及要点：利用数据的设计实现方法，对海量数据的增删改查进行处理。
7. 倒排索引(Inverted index)
适用范围：搜索引擎，关键字查询
基本原理及要点：为何叫倒排索引？一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。
以英文为例，下面是要被索引的文本：
T0 = “it is what it is”
T1 = “what is it”
T2 = “it is a banana”
我们就能得到下面的反向文件索引：
“a”: {2}
“banana”: {2}
“is”: {0, 1, 2}
“it”: {0, 1, 2}
“what”: {0, 1}
检索的条件”what”,”is”和”it”将对应集合的交集。
正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序 频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档 指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。
扩展：
问题实例：文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。
8. 外排序
适用范围：大数据的排序，去重
基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树
扩展：
问题实例：
1).有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。
这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1m做hash有些不够，所以可以用来排序。内存可以当输入缓冲区使用。
9. trie树
适用范围：数据量大，重复多，但是数据种类小可以放入内存
基本原理及要点：实现方式，节点孩子的表示方式
扩展：压缩实现。
问题实例：
1).有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。
2).1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？
3).寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。
10. 分布式处理 mapreduce
适用范围：数据量大，但是数据种类小可以放入内存
基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。
扩展：
问题实例：
1).The canonical example application of MapReduce is a process to count the appearances of
each different word in a set of documents:
2).海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。
3).一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到N^2个数的中数(median)？
经典问题分析
上千万or亿数据（有重复），统计其中出现次数最多的前N个数据,分两种情况：可一次读入内存，不可一次读入。
可用思路：trie树+堆，数据库索引，划分子集分别统计，hash，分布式计算，近似统计，外排序
所谓的是否能一次读入内存，实际上应该指去除重复后的数据量。如果去重后数据可以放入 内存，我们可以为数据建立字典，比如通过 map，hashmap，trie，然后直接进行统计即可。当然在更新每条数据的出现次数的时候，我们可以利用一个堆来维护出现次数最多的前N个数据，当 然这样导致维护次数增加，不如完全统计后在求前N大效率高。
如果数据无法放入内存。一方面我们可以考虑上面的字典方法能否被改进以适应这种情形，可以做的改变就是将字典存放到硬盘上，而不是内存，这可以参考数据库的存储方法。
当然还有更好的方法，就是可以采用分布式计算，基本上就是map-reduce过程， 首先可以根据数据值或者把数据hash(md5)后的值，将数据按照范围划分到不同的机子，
最好可以让数据划分后可以一次读入内存，这样不同的机子负责处 理各种的数值范围，实际上就是map。得到结果后，各个机子只需拿出各自的出现次数最多的前N个数据，
然后汇总，选出所有的数据中出现次数最多的前N个数 据.


##  linux
1. 硬链接 vs 软链接  
	**硬连接**
硬连接指通过索引节点来进行连接。在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为索引节点号(Inode Index)。在Linux中，多个文件名指向同一索引节点是存在的。一般这种连接就是硬连接。硬连接的作用是允许一个文件拥有多个有效路径名，这样用户就可以建立硬连接到重要文件，以防止“误删”的功能。其原因如上所述，因为对应该目录的索引节点有一个以上的连接。只删除一个连接并不影响索引节点本身和其它的连接，只有当最后一个连接被删除后，文件的数据块及目录的连接才会被释放。也就是说，文件真正删除的条件是与之相关的所有硬连接文件均被删除。  
ln命令产生硬链接。   
```ln f1 f2``` #创建 f1的一个硬链接文件f2   

	**软连接**
另外一种连接称之为符号连接（Symbolic Link），也叫软连接。软链接文件有类似于Windows的快捷方式。它实际上是一个特殊的文件。在符号连接中，文件实际上是一个文本文件，其中包含的有另一文件的位置信息。  
``` ln -s f1 f3``` #创建f1的一个符号链接文件f3

##  数据库

1. 数据库 如何在学生成绩表中查询成绩排名第五第学生成绩信息

	oracle:
	```sql
	select * from(
	select * from scrore order by score desc
	)where rownum =5
	```
	mysql : ?

2. 数据库索引的数据结构，叶子节点存储什么？非叶子节点存储什么？
	
	<http://tech.meituan.com/mysql-index.html>  
	
	<http://blog.codinglabs.org/articles/theory-of-mysql-index.html>
	
3. mysql数据库存储引擎：InnoDB和MyISAM  


	MyISAM:这个是默认类型,它是基于传统的ISAM类型,ISAM是Indexed Sequential Access Method (有索引的顺序访问方法) 的缩写,它是存储记录和文件的标准方法.与其他存储引擎比较,MyISAM具有检查和修复表格的大多数工具. MyISAM表格可以被压缩,而且它们支持全文搜索.它们不是事务安全的,而且也不支持外键。如果事物回滚将造成不完全回滚，不具有原子性。如果执行大量的SELECT，MyISAM是更好的选择。  
	InnoDB:这种类型是事务安全的.它与BDB类型具有相同的特性,它们还支持外键.InnoDB表格速度很快.具有比BDB还丰富的特性,因此如果需要一个事务安全的存储引擎,建议使用它.如果你的数据执行大量的INSERT或UPDATE,出于性能方面的考虑，应该使用InnoDB表,
对于支持事物的InnoDB类型的标，影响速度的主要原因是AUTOCOMMIT默认设置是打开的，而且程序没有显式调用BEGIN 开始事务，导致每插入一条都自动Commit，严重影响了速度。可以在执行sql前调用begin，多条sql形成一个事物（即使autocommit打开也可以），将大大提高性能。

	区别：
	是否支持外键  innoDB支持  
	是否需要事务	innoDB支持  
	是否需要全文搜索  MyISAM支持  
	经常使用什么样的查询模式（是否带where）    
	你的数据多大  innoDB  
	是否使用count(*)	MyISAM    
	主键查询	innoDB  
	insert	MyISAM  
	update	innoDB  

4. 分布式数据库如何保证一致性
5. sql注入原理
6. 数据库范式  
7. 乐观锁 vs 悲观锁
8. 数据库锁机制
9. 事务隔离机制  
10. 数据库连接池原理
11. 连接池使用什么数据结构实现，
12. B+树和二叉树查找时间复杂度
13. mysql 索引是怎么实现的？（B+树）  
	 B+ 树的大致结构 
	

##  java web

### AOP相关术语

1. 通知(Advice):
通知定义了切面是什么以及何时使用。描述了切面要完成的工作和何时需要执行这个工作。

2. 连接点(Joinpoint):
程序能够应用通知的一个“时机”，这些“时机”就是连接点，例如方法被调用时、异常被抛出时等等。  

3. 切入点(Pointcut)
通知定义了切面要发生的“故事”和时间，那么切入点就定义了“故事”发生的地点，例如某个类或方法的名称，Spring中允许我们方便的用正则表达式来指定

4. 切面(Aspect)
通知和切入点共同组成了切面：时间、地点和要发生的“故事”

5. 引入(Introduction)
引入允许我们向现有的类添加新的方法和属性(Spring提供了一个方法注入的功能）

6. 目标(Target)
即被通知的对象，如果没有AOP,那么它的逻辑将要交叉别的事务逻辑，有了AOP之后它可以只关注自己要做的事（AOP让他做爱做的事）

7. 代理(proxy)
应用通知的对象，详细内容参见设计模式里面的代理模式

8. 织入(Weaving)
把切面应用到目标对象来创建新的代理对象的过程，织入一般发生在如下几个时机:  
(1)编译时：当一个类文件被编译时进行织入，这需要特殊的编译器才可以做的到，例如AspectJ的织入编译器  
(2)类加载时：使用特殊的ClassLoader在目标类被加载到程序之前增强类的字节代码  
(3)运行时：切面在运行的某个时刻被织入,SpringAOP就是以这种方式织入切面的，原理应该是使用了JDK的动态代理技术

### sprig aop的实现方式

1. 经典的基于代理的aop

	Spring支持五种类型的通知：
Before(前) 、After-returning(返回后) 、After-throwing(抛出后) 、Arround(周围) 、Introduction(引入) 
	
	2. @Aspect注解驱动的切面
	3. 纯pojo切面
	4. 注入式AspectJ切面
2. spring 事务如何实现
3. spring事务遇到什么异常的时候不回回滚  
	
	Spring的事务管理默认只对出现运行期异常(java.lang.RuntimeException及其子类)进行回滚。

4. 讲讲web三大框架
5. 什么技术是关于解藕的	
6. spring特性，为什么使用spring
7. aop 和 ioc介绍
8. spring 3 和以前的版本有什么区别吗
9. 注解相关问题：使用，原理
10. 项目中怎么保存用户登录信息，如果cookie禁用了呢  
11. forward转发与 redirect重定向的区别  

	转发：浏览器不知道，新页面继续处理同一个请求，表现在浏览器地址栏地址未改变。  
	重定向：浏览器地址改变，第一个请求发出后，返回信息带着新的请求地址，
1. hibernate缺点  
4. spring事务机制  
6. 项目登陆过程如何保证安全  
7. 在线支付系统，和银行对接如何保证安全 
8. servlet生命周期
9. html访问全过程
1. 你对mvc的理解

	>M是指数据模型，V是指用户界面，C则是控制器。使用MVC的目的是将M和V的实现代码分离，从而使同一个程序可以使用不同的表现形式。
19. mybatis 的 $ 与 # 的区别？	

	>他们两都可以来传递参数，不过 # 可以方式 sql 注入，而 $ 就是字符串拼接的方式处理，可能会有sql 注入的问题。那就是 #{} 在预处理时，会把参数部分用一个占位符 ? 代替。




##  算法

1. 单向链表存在环结构，如何快速判断存在环结构

	一块一慢两个指针

2. 单向链表，存在一个特定的节点，如何快速删除该节点

3. 编程题 1,11,21,1211,111221,312211,13112221...
	数列规律，后面的数字是对前一个数字的描述，第二个数字11表示1个1，描述的是第一个数字。21表示2个1，1211表示1个2，1个1。  

4. 字符串逆序 	
5. 快速排序

	快速排序运用了分治的思想、递归的思想。将数组分成两部分，两部分分别排序
	
	```python
	def quick_sort(arr,start=0,end=None):
    if end is None:
        end = len(arr)-1
    if end<=start:
        return(arr)
    i,j = start,end
    ref = arr[start]
    while j>i:
        if arr[j]>= ref:
            j = j - 1
        else:
            # 此处使用一行语句交换3个元素的值
            arr[i],arr[j],arr[i+1] = arr[j],arr[i+1],arr[i]
            i = i + 1
    quick_sort(arr,start=start,end = i-1)
    quick_sort(arr,start=i+1,end = end)
    return(arr)

	print(quick_sort([1,1,3,3,2,2,6,6,6,5,5,7]))
	
	result:  
	[1, 1, 2, 2, 2, 3, 5, 5, 6, 6, 6, 7]
	```
	
6. 二分查找
	
	题目形式：手写一下二分查找算法。给定一个有序数组 arr 和一个目标元素 target ，返回该 target 在 arr 中的索引，若不存在，返回-1。  
	题目难度：简单。  
	思想：二分查找有两个游标，每次游标都朝着减半的位置移动。但是二叉查找有个限制，就是待查找的数组必须是有序的。
	
	```python
	def binary_search(arr,target):
	    star,end = 0,len(arr)-1
	    while True:
	        if end - start <= 1:
	            if target == arr[start]:
	                return(start)
	            elif target == arr[end]:
	                return(end)
	            else:
	                return(-1)
	         mid = (start + end)//2
	         if arr[mid]>=target:
	             end = mid
	         else:
	             start = mid
	demo:
	print(binary_search([1,4,7,8,9,12],9))
	print(binary_search([1,4,7,8,9,12],3))
	result:
	4
	-1
	```
	
6. 爬楼梯

	题目形式：有一个楼梯，总共有10级台阶，每次只能走一级或者两级台阶，全部走完，有多少种走法？  
	题目难度：简单。  
	爬楼梯问题可以用递归来解决，但是如果考虑到时间复杂度，最好用动态规划的思想来处理，这是一个动态规划算法的教科书级别的案例。  
	[关于爬楼梯算法分析](关于爬楼梯算法分析)
	
	```python
	def climb_stairs(n):
    if n==1:
        return 1
    if n==2:
        return 2
    a,b = 1,2
    i = 3
    while i<=n:
        a,b = b,a+b
        i +=1
    return b

	print(climb_stairs(10))
	result:89
	```

6. 两数之和

	题目形式：寻找列表中满足两数之和等于目标值的元素的下标。例如：arr = [2,7,4,9]，target = 6  则返回 [0,2]，若不存在，返回空列表[]。

	题目难度：简单。  
	两数之和问题考察候选人对哈希表可以用空间换取时间这个特性的理解。
	
	```python
	def sum_of_two(arr,target):
    dic = {}
    for i,x in enumerate(arr):
        j = dic.get(target-x,-1)
        if j != -1:
            return((j,i))
        else:
            dic[x] = i
    return([])

	arr = [2,7,4,9]
	target = 6
	print(sum_of_two(arr,target))
	result:(0, 2)
	```

6. 最大回撤
6. 合并两个有序数组
6. 最大连续子数组和
6. 最长不重复子串
6. 全排列
6. 三数之和

##  设计模式

[设计模型代码展示repo](https://github.com/caychan/DesignPattern)

##  网络


1. get post put detele区别

	Http定义了与服务器交互的不同方法，最基本的方法有4种，分别是GET，POST，PUT，DELETE。分别对应查、改、增、删四个操作。GET一般用于获取/查询资源信息，而POST一般用于更新资源信息。  
	get用于信息获取，而且应该是安全的和幂等的。所谓安全的意味着该操作用于获取信息而非修改信息。换句话说，GET 请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。幂等的意味着对同一URL的多个请求应该返回同样的结果。  
	根据HTTP规范，POST表示可能修改变服务器上的资源的请求。读者对新闻发表自己的评论应该通过POST实现，因为在评论提交后站点的资源已经不同了，或者说资源被修改了。
	  put、delete操作是幂等的。
	  
	**post和put区别:**    
	创建操作可以使用POST，也可以使用PUT，区别在于POST 是作用在一个集合资源之上的（/uri），而PUT操作是作用在一个具体资源之上的（/uri/xxx），再通俗点说，如果URL可以在客户端确定，那么就使用PUT，如果是在服务端确定，那么就使用POST，比如说很多资源使用数据库自增主键作为标识信息，而创建的资源的标识信息到底是什么只能由服务端提供，这个时候就必须使用POST。  
	
	对资源的增，删，改，查操作，其实都可以通过GET/POST完成，不需要用到PUT和DELETE。
	另外一个是，早期的Web MVC框架设计者们并没有有意识地将URL当作抽象的资源来看待和设计，所以导致一个比较严重的问题是传统的Web MVC框架基本上都只支持GET和POST两种HTTP方法，而不支持PUT和DELETE方法。    
	
	**提交数据:**GET请求的数据会附在URL之后（就是把数据放置在HTTP协议头中），以?分割URL和传输数据，参数之间以&相连。如果是空格，转为为＋，如果是中文/其他字符，则直接把字符串用BASE64加密。POST把提交的数据则放置在是html header内一起传送到action属性所指的url地址。  
	
	**接收数据:**  对于get方式，服务器端用Request.QueryString获取变量的值，对于post方式，服务器端用Request.Form获取提交的数据。   
	
	
	
	**请求数据大小:**
	GET是通过url提交数据，get可提交的数据量就跟url长度有关系了。URL不存在参数上限的问题，HTTP协议规范没有对URL长度进行限制。这个限制是特定的浏览器及服务器对它的限制。  
	POST是没有大小限制的，HTTP协议规范也没有进行大小限制，说“POST数据量存在80K/100K的大小限制”是不准确的，POST数据是没有限制的，起限制作用的是服务器的处理程序的处理能力。


2. 表单信息传到服务器加密问题
	HTTPS 对称加密 非对称加密 tls/ssl rsa
	
	
3. session vs cookie  

	http是无协议的，客户请求第二次到达服务器，服务器如何分辩此用户之前访问过服务器，用户的信息是如何保存的。  
	cookie是在客户端保存客户的状态。包括（名字，值，过期时间，路径，域）  
	session是在服务端保持状态。  
	服务器程序为客户端的请求创建session时，服务器首先检查这个客户端的请求里是否包含sessionID。  
	如果客户端禁用cookie，使用url重写、表单隐藏字段端方法传递session值。

4. dns是基于tcp还是udp的  
  udp。在数据传输时间很短，以至于此前的连接过程成为整个流量主体的情况下，UDP也是一个好的选择。
5. 使用过翻墙工具吗，goagent怎么实现的，shadowsocks怎么实现的  
6. HTTP HTTPS的区别？前台数据加密传输后台  
7. html提交如何保证安全  
8. https如何加密的，怎么做到安全的  
9. get提交到字节限制是协议本身限制的吗

	http协议本身没有限制get提交字节长度，是url长度的限制。因为get提交到数据要放到url中。
	
10. 计算机网络分层，每层所用协议，协议所占端口
11. osi七层模型 vs tcp/ip四层模型，两者之间有什么区别
12. HTTP method

  1. 一台服务器要与HTTP1.1兼容，只要为资源实现GET和HEAD方法即可  
  2. GET是最常用的方法，通常用于请求服务器发送某个资源。  
  3. HEAD与GET类似，但服务器在响应中值返回首部，不返回实体的主体部分  
  4. PUT让服务器用请求的主体部分来创建一个由所请求的URL命名的新文档，或者，如果那个URL已经存在的话，就用干这个主体替代它。  
  5. POST起初是用来向服务器输入数据的。实际上，通常会用它来支持HTML的表单。表单中填好的数据通常会被送给服务器，然后由服务器将其发送到要去的地方。  
  6. TRACE会在目的服务器端发起一个环回诊断，最后一站的服务器会弹回一个TRACE响应并在响应主体中携带它收到的原始请求报文。TRACE方法主要用于诊断，用于验证请求是否如愿穿过了请求/响应链。  
  7. OPTIONS方法请求web服务器告知其支持的各种功能。可以查询服务器支持哪些方法或者对某些特殊资源支持哪些方法。  
  8. DELETE请求服务器删除请求URL指定的资源。  
  
13. TCP 与 UDP的区别  
	TCP(传输控制协议，Transmission Control Protocol)
	UDP(用户数据报协议，User Data Protocol)
	连接：TCP是面向连接的协议，正式通信前必须要与对方建立连接。UDP面向非连接的协议，正式通信前不必与对方建立连接，不管对方状态就直接发送。    
	传输可靠性：TCP传输更可靠，保证数据正确性,保证数据顺序。UDP传输不可靠。不能提供可靠性、流控、差错恢复功能。
	应用场合：TCP适合一对一传送数据，适合传输大量数据。UDP适合广播数据，适合少量数据，e.g.ping
	速度：TCP慢，因为要建立连接。UDP快。
14. TCP三次握手  

  TCP三次握手的过程如下：  
 - 客户端发送SYN（SEQ=x）报文给服务器端，进入SYN_SEND状态。  
 - 服务器端收到SYN报文，回应一个SYN （SEQ=y）ACK(ACK=x+1）报文，进入SYN_RECV状态。  
 - 客户端收到服务器端的SYN报文，回应一个ACK(ACK=y+1）报文，进入Established状态。    
 ![alt图片](http://c.hiphotos.baidu.com/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=d30689c3828ba61ecbe3c07d205dfc6f/29381f30e924b899cb32f6316e061d950a7bf6a9.jpg)  

15. TCP四次挥手   

 - 用进程首先调用close，称该端执行“主动关闭”（active close）。该端的TCP于是发送一个FIN分节，表示数据发送完毕。  
 - 接收到这个FIN的对端执行 “被动关闭”（passive close），这个FIN由TCP确认。  
注意：FIN的接收也作为一个文件结束符（end-of-file）传递给接收端应用进程，放在已排队等候该应用进程接收的任何其他数据之后，因为，FIN的接收意味着接收端应用进程在相应连接上再无额外数据可接收。  
 - 一段时间后，接收到这个文件结束符的应用进程将调用close关闭它的套接字。这导致它的TCP也发送一个FIN。  
 - 接收这个最终FIN的原发送端TCP（即执行主动关闭的那一端）确认这个FIN。[1]   
既然每个方向都需要一个FIN和一个ACK，因此通常需要4个分节。  
  ![alt图片](http://e.hiphotos.baidu.com/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=5e4b5e36eb50352aa56c2d5a322a9097/4610b912c8fcc3cea74598b29045d688d53f20ad.jpg)  
  
16. HTTP返回消息代码含义  

	1xx：信息  
	2xx：成功  
	3xx：重定向  
	4xx：客户端错误  
	5xx：服务器错误


##  操作系统

1. 与进程的区别:  
(1)地址空间:进程内的一个执行单元;进程至少有一个线程;它们共享进程的地址空间;而进程有自己独立的地址空间。  
(2)资源拥有:进程是资源分配和拥有的单位,同一个进程内的线程共享进程的资源。  
(3)线程是处理器调度的基本单位,但进程不是。  

2. 什么是可重入锁
	java 递归锁，可重入锁
所谓递归锁，就是在同一线程上该锁是可重入的，对于不同线程则相当于普通的互斥锁。
func A （） {  
     LOCK.lock();
     B();
    LOCK.unlock();
  }

  func B（） {
     LOCK.lock();
     LOCK.unlock();
  }
  java为每个线程分配一个锁，而不是为每次调用分配一个锁。
   java锁的可重入性机制可以解决下面这个问题
   可重入的意思是线程可以重复获得它已经持有的锁。
 
3. 对线程安全的理解

## java 基础  

1. hashmap的底层实现，如何处理冲突，采用链表存储同一个hash值的元素，对于什么操作会有影响。

	底层实现是数组，数组中的对象是Entry类对象。  
	当存在冲突时，将具有相同hashcode的对象挂载在相同槽内，组成链。采用头插法，新插入对象放在链头，最先加入的对象放在链尾。  
	**hashcode()**  
	**indexFor()**  
	**put()**  
	**get()**  
	**addEntry()**  
	**loadfactor**  
	loadfactor=元素个数/表长  
	**hash算法**  

	```java
	int indexFor(int h, int length){
		return h&(length - 1)	
	}
	```
	**resize**  
	数组默认大小是16，负载因子默认未0.75  

2. 为何重写hashcode 和 equals方法  
	重写equals方法必然重写hashcode方法，这是java规范。  
	hashcode返回值是一个整数，代表两个对象是否相等。  


3. 异常种类有哪些
	
	![java中常见异常如下图所示](img/exception_1.jpg)  
	Throwable是所有异常的根，java.lang.Throwable  
	Error是错误，java.lang.Error  
	Exception是异常，java.lang.Exception  
	一般分为Checked异常和Runtime异常，所有RuntimeException类及其子类的实例被称为Runtime异常，不属于该范畴的异常则被称为CheckedException。


4. [java 排序算法代码实现](http://www.cnblogs.com/wolf-sun/p/4312475.html)，复杂度、稳定性

5. 浅拷贝 vs 深拷贝

	**浅拷贝**是指拷贝对象时仅仅拷贝对象本身（包括对象中的基本变量），而不拷贝对象包含的引用指向的对象。  
	**深拷贝**不仅拷贝对象本身，而且拷贝对象包含的引用指向的所有对象。  
	若不对clone()方法进行改写，则调用此方法得到的对象即为浅拷贝。  
	[详细解释浅拷贝与深拷贝](http://www.cnblogs.com/shuaiwhu/archive/2010/12/14/2065088.html)  
	当然我们还有一种深拷贝方法，就是将对象串行化。缺点耗时。

6. 怎么判断一个对象该被回收  
	计数法  
	根搜索法

7. hashMap 和 hashtable区别
8. hashtable原理
9. hashtable是怎么实现线程安全的
8. java的代理是怎么实现的
9. 抽象类和接口的区别
10. 接口和实现类的区别
10. java四种引用
11. 集合类介绍，各种集合类之间的区别
12. 继承和组合区别
13. lmbda表达式
14. java 8 新特性
15. java基础数据类型 
	- 整形 byte	short	int		long  
	- 浮点型	float	double  
	- 逻辑型 boolean  
	- 字符型	char  
16. java中的数据存储  

	- 寄存器：最快的存储区，编译器分配，程序无法控制
	- 栈：存放基础数据类型的变量数据 和 对象的引用  
	- 堆：new出来的对象存放位置
	- 静态域：静态成员变量存放位置  
	- 常量池：字符串变凉&基本数据类型变量 public static final  
	- 非rRAM存储：硬盘等永久存储空间
20. 内存溢出 和 内存泄漏
21. 内存敲代码优化，我们都用stringbuffer来代替+号，来减少建立的对象。那么问题来了，除了这个你还用过什么碼代码技巧来节省内存 
22. 方法走完，引用消失，堆内存未必消失，好多人在做报表导出的时候，就会在for循环里不断的创建对象，很容易造成堆溢出，请问这种大文件导出怎么破？  
  建议不要在for里创建对象，可以在外面搞一个对象，for循环里对一个对象修改数据即可。
23. java支持多线程，每个线程有自己的java虚拟机栈和本地方法栈。✔️
24. 新建的实例在堆内存，实例变量也是在堆内存。✔️
25. 入栈和出栈  
	入栈的时候，就是执行一个方法的时候，为这个方法创建一个栈帧入栈。  
	出栈的时候，就是方法执行完毕了。  
26. 加载父类->初始化父类->加载子类  
27. 如果我有一个静态的成员变量int，那我多线程更改是否会有线程安全问题，为什么？
	静态成员变量，它在内存里， 只有一份，就是属于类的。你多个线程并发修改，一定会有并发问题，可能导致数据出错。 
28. 类加载是按需加载，可以一次性加载全部的类吗？
	如果是默认的类加载机制，那么是你得代码运行过程中，遇到什么类加载什么类。  
	如果你要自己加载类，那么需要写自己的类加载器。  
29. 多线程的实现方式有哪些？
    继承 Thread 类、实现Runnable 接口，最后调用 的是 start() 方法来启动线程。  
    start() 跟 run() 方法的区别和联系?  
    直接调用 start() 方法，此时线程处于一个就绪（可运行）的状态，但是并没有真正	的运行。而是得到CPU 的时间片后，开始执行 run() 方法，run() 方法里面的是我们	的线程体。
	我们直接 运行 run() 方法，它其实就是一个普通的方法调用，在主线程中执行，是不会开启多线程的。
30. 死锁   
	**产生死锁的原因主要是**：  
（1） 因为系统资源不足。  
（2） 进程运行推进的顺序不合适。  
（3） 资源分配不当等。   
    **产生死锁的四个必要条件**：  
（1）互斥条件：一个资源每次只能被一个进程使用。  
（2）占有且等待：一个进程因请求资源而阻塞时，对已获得的资源保持不放。   
（3）不可强行占有:进程已获得的资源，在末使用完之前，不能强行剥夺。  
（4）循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。   
这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之
一不满足，就不会发生死锁。 
 
	**处理死锁的基本方法**（银行家算法）：

	- 死锁预防：通过设置某些限制条件，去破坏死锁的四个条件中的一个或几个条件，来预防发生死锁。但由于所施加的限制条件往往太严格，因而导致系统资源利用率和系统吞吐量降低。
	- 死锁避免：允许前三个必要条件，但通过明智的选择，确保永远不会到达死锁点，因此死锁避免比死锁预防允许更多的并发。
	- 死锁检测：不须实现采取任何限制性措施，而是允许系统在运行过程发生死锁，但可通过系统设置的检测机构及时检测出死锁的发生，并精确地确定于死锁相关的进程和资源，然后采取适当的措施，从系统中将已发生的死锁清除掉。
	- 死锁解除：与死锁检测相配套的一种措施。当检测到系统中已发生死锁，需将进程从死锁状态中解脱出来。常用方法：撤销或挂起一些进程，以便回收一些资源，再将这些资源分配给已处于阻塞状态的进程。死锁检测盒解除有可能使系统获得较好的资源利用率和吞吐量，但在实现上难度也最大。

	**处理死锁的基本算法**    
	1. 如果request<=need，转向步骤2；否则认为出错，因为请求资源大于需要资源。
	2. 如果request<=available，转向步骤3,；否则尚无足够资源，进程p阻塞；
	3. 系统尝试为把资源分配给进程P，并修改available、allocation和need的数值。
	4. 系统执行安全性算法，检查此次分配后系统是否处于安全状态，若安全，才正式将资源分配给进程P，否则将本次试探性分配作废，让进程P等待。  
	安全状态：系统能按照某种进程顺序，为每个进程分配资源，直至满足每个进程对资源的最大需求，使每个进程都可顺利完成。
	
31. 线程池  
    newSingleThreadExecutor、  
    newFixedThreadPool、  
    newCachedThreadPool、  
    newScheduledThreadPool  
线程池底层都是通过 ThreadPoolExecutor 来实现的

	```java
	public ThreadPoolExecutor
	( 
	int corePoolSize,                          
	int maximumPoolSize,                         
	long keepAliveTime,                           
	TimeUnit unit,             
	BlockingQueue <Runnable> workQueue,                     
	ThreadFactory  threadFactory,              
	RejectedExecutionHandler handler)
	```
	几个参数的意思分别为：

	- corePoolSize： 线程池里最小线程数
	- maximumPoolSize：线程池里最大线程数量，超过最大线程时候会使用 RejectedExecutionHandler
	- keepAliveTime：线程最大的存活时间，超过这个时间就会被回收
	- unit：线程最大的存活时间的单位
	- workQueue：缓存需要执行的异步任务的队列
	- threadFactory：新建线程工厂
	- handler：拒绝策略，表示当workQueue已满，且池中的线程数达到maximumPoolSize时，线程池拒绝添加新任务时采取的策略。DiscardPolicy：抛弃当前任务，DiscardOldestPolicy：扔掉最旧的，CallerRunsPolicy：由向线程池提交任务的线程来执行该任务，AbortPolicy：抛出 RejectedExecutionException 异常。
32. 这几种线程池在哪些情况下使用什么类型的
33. 如何判断两个对象是否相等


# 应用
##  实际问题

2. xml和json的区别，哪个流量比较大
3. 如何快速查出你当前所在地最近的一百家餐馆（不能用遍历）
4. 海量数据查处每天访问百度网站最多的前100个人的ip地址
5. 如何获取到美团网页上商家的名称、地址、电话号码
8. 网站访问量巨大，如何提高效率
1. 设计qq登录系统，10个服务器，8亿用户2亿用户是活跃的，怎么让登录系统既快又稳定  
2. 用户不多时候，一个服务器对应一个数据库。用户多了的时候就是n个服务器对应一个数据库，这样会出现什么问题，怎么解决 

##  [面试实例1](demo01.md) 







	










	



 




 